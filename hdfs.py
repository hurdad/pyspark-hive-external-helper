"""
HDFS helper module.
"""

__author__ = "jonathag"

logging = __import__("logging")
datetime = __import__("datetime")
os = __import__("os")
sh = __import__("sh")

logger = logging.getLogger(__name__)


def hdfs(*args, **kwargs):
    """Run an HDFS command.

    Returns an sh.RunningCommand object, which can be queried for return code,
    stdout and stderr.  Refer to ``sh`` module documentation for more details.

    .. _sh Module Documentation:
        https://amoffat.github.io/sh/

    All positional and named parameters will be passed to the ``hdfs dfs`` command.

    Args:
        *args: Positional arguments to pass to ``hdfs dfs`` command.
        **kwargs: Keyword arguments to pass to ``hdfs dfs`` command.

    >>> import dunnhumby
    >>> result = dunnhumby.utils.hdfs.hdfs('-test', '-d', '/')
    >>> print result.exit_code
    0
    >>> print result.stdout
    """
    if logging.getLogger('sh').level < 20:
        logging.getLogger('sh').setLevel(logging.INFO)

    cmd = sh.Command('hdfs')
    cmd = cmd.bake("dfs")

    logger.debug('Running hdfs command: %s' % cmd + ' ' + ' '.join(args))

    result = cmd(*args, **kwargs)

    logger.debug('Command return code: %s', result.exit_code)

    return result


def test(*args):
    """Perform hdfs -test operation."""
    return hdfs('-test', *args)


def exists(type_, file_):
    """Check if an HDFS file or directory exists.

    type    File or directory, use -f or -d

    file    HDFS path to file or directory
    """
    try:
        test(type_, file_)
        return True
    except sh.ErrorReturnCode_1:
        return False


def fileexists(file_):
    """Check if an HDFS file exists."""
    logger.debug('Checking if file "{}" exists'.format(file_))
    return exists('-f', file_)


def direxists(dir_):
    """Check if an HDFS directory exists."""
    logger.debug('Checking if directory "{}" exists'.format(dir_))
    return exists('-d', dir_)


def mkdir(directory, perms=None, fail_if_exists=True):
    """Create an HDFS directory.

    Optionally specify directory permissions to be applied.
    If no permissions are specified then they will be left as
    the default.

    Args:
        directory (str): Directory to be created.
        perms (str): Octal permissions, e.g. 775.
        fail_if_exists (bool, optional): If True, fail if
            directory already exists.
    """
    args = ['-mkdir', directory]

    if not fail_if_exists:
        args.insert(1, '-p')

    hdfs(*args)

    if perms:
        hdfs('-chmod', perms, directory)


def rmdir(dir_):
    """
    Remove an empty HDFS directory.

    Args:
        dir_ (str): Name of empty directory to remove.
    """
    return hdfs('-rmdir', dir_)


def rm(*args, **kwargs):
    """Delete a file or directory.

    Example:

        >>> import dunnhumby
        >>> result = dunnhumby.utils.hdfs.rm('/some/directory/somefile.txt')
        >>> result = dunnhumby.utils.hdfs.rm('-r', '/some/directory')
    """
    return hdfs('-rm', *args, **kwargs)


def put(srcfile, tgtfile):
    """Put a local file onto HDFS.

    Args:
        srcfile (str): Local source file.
        tgtfile (str): Target file location on HDFS.
    """
    return hdfs('-put', srcfile, tgtfile)


def cat(file_):
    """
    Cat a local file on HDFS

    Args:
        file_ (str): File to retrieve contents
    """
    return hdfs('-cat', file_)


def mv(oldPath_, newPath_, **kwargs):
    """
    Move a local file on HDFS

    Args:
        oldPath_ (str): The full path to the old file
        newPath_ (str): The full path to the new file
    """
    return hdfs('-mv', oldPath_, newPath_, **kwargs)


def find(*args, **kwargs):
    """
    Find hdfs files matching ``path``.

    Args:
        ignore_errors (bool, optional): Ignore if the hdfs command returns
            with error code 1. This is common if the find command
            encounters a directory where the user does not have permissions.
        stderr_logger: A function to be used for stderr messages
            generated by the hdfs command.  Default is ``logging.warning``.
            Logging of stderr messages can be disabled by passing ``stderr_logger``
            as ``None``.

            If you want to continue using the ``hdfs`` modules logger
            then you will need to pass the logger like this:

            >>> find('/nonexistendir', ignore_errors=True,
            ...      stderr_logger=dunnhumby.utils.hdfs.logger.error)

        *args: Any arguments valid in ``hdfs dfs -find`` command. Include each
            item as a separate parameter. Do not include `-print0`. This will
            be included automatically.
        **kwargs: Any keyword arguments valid in ``hdfs dfs -find`` command.
            Do not include ``_ok_code``.  This will be included automatically.

    Returns:
        :obj:`list` of :obj:`str`): A list of matching files.
    """
    # Look for out parameters and pull them out of the keyword dict.
    ignore_errors = False
    stderr_logger = logger.warning

    if 'ignore_errors' in kwargs:
        ignore_errors = kwargs['ignore_errors']
        del kwargs['ignore_errors']

    if 'stderr_logger' in kwargs:
        stderr_logger = kwargs['stderr_logger']
        del kwargs['stderr_logger']

    # Create the hdfs command argument list.
    args = ['-find'] + list(args) + ['-print0']

    # Create a list of return codes to accept as OK
    ok_code_list = [0]
    if ignore_errors:
        ok_code_list.append(1)

    # Run the command with _ok_code option.
    results = hdfs(*args, _ok_code=ok_code_list, **kwargs)

    # Capture stderr lines, split on newline and remove blank lines
    stderr_lines = results.stderr.rstrip('\n').split('\n')

    # Display errors at the appropriate logging level.
    if stderr_logger:
        for line in stderr_lines:
            stderr_logger(line)

    logger.debug('Result raw stdout: %s', str(results))

    # There is commonly an extra empty element in the return list
    # which can be removed with rstrip.
    # Split the results on null character
    return_list = results.rstrip('\x00').split('\x00')

    if len(return_list) == 1 and str(return_list[0]) == '':
        return_list = []

    return return_list


def ls(*args):
    """
    List files and directories on hdfs
    """
    return hdfs('-ls', *args)


def file_statistics(hdfs_listing):
    """
    Extracts the fields for each file in a raw HDFS "ls" listing and returns them as a list of Dict instances,
    one per file.
    """
    result = []
    for line in hdfs_listing.rstrip().splitlines():
        fields = line.split()
        if len(fields) < 8:
            continue

        # -rwxrwx---+  3 pankajk hive     197353 2016-09-21 21:25 /myclient/tbl/date=1/part-00000
        result.append({"isfile": fields[0][0] == "-", "permissions": fields[0],
                       "owner": fields[2], "group": fields[3], "size": int(fields[4]),
                       "last_modified": datetime.datetime.strptime(fields[5] + " " + fields[6], "%Y-%m-%d %H:%M"),
                       "path": fields[7]})
    return result


def chmod(*args, **kwargs):
    """Perform hdfs -chmod operation."""
    return hdfs('-chmod', *args, **kwargs)


def du(*args, **kwargs):
    """Perform hdfs -du operation."""
    return hdfs('-du', *args, **kwargs)

